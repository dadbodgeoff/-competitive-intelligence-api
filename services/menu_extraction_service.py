#!/usr/bin/env python3
"""
Menu Extraction Service - PRODUCTION IMPLEMENTATION
Real menu extraction from competitor websites using multiple strategies
"""
import logging
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import asyncio
import json
import re
from bs4 import BeautifulSoup
import google.generativeai as genai
import os
import requests
from dotenv import load_dotenv

# Import utilities
from services.menu_scraping_utils import menu_utils

# Load environment variables
load_dotenv()

logger = logging.getLogger(__name__)

class MenuExtractionService:
    """
    Production menu extraction service
    
    Extraction priority order (cost-optimized):
    1. Toast ordering platforms (85% success, $0.02)
    2. Square websites (70% success, $0.02)  
    3. Slice platforms (80% success, $0.03)
    4. Google Vision API (95% success, $0.04)
    """
    
    def __init__(self):
        self.extraction_methods = [
            'toast',      # Toast ordering platforms - HIGHEST PRIORITY
            'square',     # Square websites - SECOND PRIORITY
            'slice',      # Slice ordering platforms - THIRD PRIORITY
            'vision',     # Google Vision API - LAST RESORT
        ]
        
        # Initialize Gemini for Vision API text structuring
        self.gemini_api_key = os.getenv("GOOGLE_GEMINI_API_KEY")
        if self.gemini_api_key:
            genai.configure(api_key=self.gemini_api_key)
            self.gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
        else:
            self.gemini_model = None
            logger.warning("Gemini API key not found - Vision fallback will be limited")
        
        logger.info("MenuExtractionService initialized with real extraction methods")
    
    async def extract_all_menus(
        self, 
        competitors: List, 
        tier: str = "free"
    ) -> List[Dict]:
        """
        Extract menus from all competitors using real extraction methods
        
        Args:
            competitors: List of competitor objects from GooglePlacesService
            tier: "free" or "premium" (affects number of competitors)
            
        Returns:
            List of extracted menu data dictionaries
        """
        
        logger.info(f"Starting menu extraction: {len(competitors)} competitors, tier={tier}")
        
        # Limit competitors based on tier
        max_competitors = 2 if tier == "free" else 5
        limited_competitors = competitors[:max_competitors]
        
        extracted_menus = []
        
        for i, competitor in enumerate(limited_competitors):
            logger.info(f"ðŸ” Extracting menu for competitor {i+1}/{len(limited_competitors)}: {competitor.name}")
            
            try:
                menu_data = await self.extract_competitor_menu(competitor)
                if menu_data and menu_data.get('success'):
                    extracted_menus.append(menu_data)
                    logger.info(f"âœ… Successfully extracted menu via {menu_data.get('extraction_method')}")
                else:
                    logger.warning(f"âŒ Failed to extract menu for {competitor.name}")
                    
            except Exception as e:
                logger.error(f"ðŸ’¥ Exception extracting menu for {competitor.name}: {e}")
                continue
        
        success_rate = len(extracted_menus) / len(limited_competitors) * 100 if limited_competitors else 0
        logger.info(f"ðŸ“Š Menu extraction complete: {len(extracted_menus)}/{len(limited_competitors)} successful ({success_rate:.1f}%)")
        
        return extracted_menus
    
    async def extract_competitor_menu(self, competitor) -> Dict:
        """
        Extract menu from a single competitor using fallback chain
        
        Args:
            competitor: Competitor object with name, place_id, website, etc.
            
        Returns:
            Dictionary with extraction results and menu data
        """
        
        logger.info(f"ðŸŽ¯ Starting extraction for: {competitor.name}")
        
        # Try each extraction method in priority order
        for method_name in self.extraction_methods:
            try:
                logger.info(f"ðŸ”„ Trying {method_name} extraction for {competitor.name}")
                
                # Get the extraction method
                method_func = getattr(self, f'_extract_via_{method_name}')
                
                # Attempt extraction
                menu_data = await method_func(competitor)
                
                if menu_data and menu_utils.validate_menu_quality(menu_data):
                    logger.info(f"âœ… {method_name} extraction successful for {competitor.name}")
                    
                    # Normalize the menu structure
                    normalized_menu = menu_utils.normalize_menu_structure(menu_data, competitor.name)
                    
                    return {
                        'competitor_name': competitor.name,
                        'competitor_id': competitor.place_id,
                        'extraction_method': method_name,
                        'extraction_url': self._get_extraction_url(competitor, method_name),
                        'success': True,
                        'menu_data': normalized_menu,
                        'extracted_at': datetime.now().isoformat(),
                        'item_count': self._count_menu_items(normalized_menu)
                    }
                else:
                    logger.info(f"âš ï¸ {method_name} extraction failed quality check for {competitor.name}")
                
            except Exception as e:
                logger.warning(f"âŒ {method_name} extraction failed for {competitor.name}: {e}")
                continue
        
        # All methods failed
        logger.error(f"ðŸ’¥ All extraction methods failed for {competitor.name}")
        return {
            'competitor_name': competitor.name,
            'competitor_id': competitor.place_id,
            'extraction_method': 'failed',
            'success': False,
            'menu_data': None,
            'extracted_at': datetime.now().isoformat(),
            'error': 'All extraction methods failed'
        }
    

    
    def _count_menu_items(self, menu_data: Dict) -> int:
        """Count total items in menu"""
        if not menu_data or not isinstance(menu_data, dict):
            return 0
        
        total_items = 0
        for category in menu_data.get('categories', []):
            if isinstance(category, dict):
                total_items += len(category.get('items', []))
        return total_items
    
    def _get_extraction_url(self, competitor, method_name: str) -> Optional[str]:
        """Get the URL used for extraction based on method"""
        try:
            if method_name == 'toast':
                return self._find_toast_url(competitor)
            elif method_name == 'square':
                return self._find_square_url(competitor)
            elif method_name == 'slice':
                return self._find_slice_url(competitor)
            elif method_name == 'vision':
                return f"Google Places Photos for {competitor.place_id}"
            else:
                return None
        except:
            return None
    
    # ============================================================================
    # TOAST EXTRACTION (HIGHEST PRIORITY - 85% SUCCESS RATE)
    # ============================================================================
    
    async def _extract_via_toast(self, competitor) -> Optional[Dict]:
        """
        Extract menu from Toast ordering platform
        
        Toast platforms use structured JSON-LD data and have consistent HTML structure
        Success rate: 85% when Toast URL is found
        Cost: $0.02 (HTTP requests only)
        """
        
        toast_url = self._find_toast_url(competitor)
        if not toast_url:
            logger.debug(f"No Toast URL found for {competitor.name}")
            return None
        
        logger.info(f"ðŸž Found Toast URL: {toast_url}")
        
        try:
            # Fetch HTML content
            html = await menu_utils.fetch_html(toast_url)
            if not html:
                logger.warning(f"Failed to fetch Toast HTML for {competitor.name}")
                return None
            
            # Method 1: Try JSON-LD structured data (most reliable)
            json_ld_objects = menu_utils.extract_json_ld(html)
            for json_obj in json_ld_objects:
                if self._is_toast_menu_data(json_obj):
                    menu = self._parse_toast_json_ld(json_obj)
                    if menu:
                        logger.info(f"âœ… Toast JSON-LD extraction successful")
                        return menu
            
            # Method 2: Parse Toast HTML structure
            soup = BeautifulSoup(html, 'html.parser')
            menu = self._parse_toast_html(soup)
            if menu:
                logger.info(f"âœ… Toast HTML extraction successful")
                return menu
            
            # Method 3: Look for Toast API data in script tags
            menu = self._extract_toast_api_data(soup)
            if menu:
                logger.info(f"âœ… Toast API data extraction successful")
                return menu
            
            logger.warning(f"Toast extraction found URL but no menu data for {competitor.name}")
            return None
            
        except Exception as e:
            logger.error(f"Toast extraction error for {competitor.name}: {e}")
            return None
    
    def _find_toast_url(self, competitor) -> Optional[str]:
        """
        Find Toast ordering URL for competitor
        
        Toast URL patterns:
        - order.toasttab.com/online/{restaurant-slug}
        - {restaurant}.toasttab.com
        """
        
        # Method 1: Check if website is already a Toast URL
        if hasattr(competitor, 'website') and competitor.website:
            if 'toasttab.com' in competitor.website:
                logger.debug(f"Found direct Toast URL in website: {competitor.website}")
                return competitor.website
        
        # Method 2: Generate likely Toast URL from restaurant name
        restaurant_slug = menu_utils.generate_restaurant_slug(competitor.name)
        if not restaurant_slug:
            return None
        
        # Common Toast URL patterns
        toast_patterns = [
            f"https://order.toasttab.com/online/{restaurant_slug}",
            f"https://{restaurant_slug}.toasttab.com",
            f"https://order.toasttab.com/online/{restaurant_slug}-restaurant",
            f"https://order.toasttab.com/online/{restaurant_slug}pizza",  # For pizza places
        ]
        
        # Test each pattern
        for url in toast_patterns:
            logger.debug(f"Testing Toast URL: {url}")
            if menu_utils.url_exists(url):
                logger.info(f"âœ… Found working Toast URL: {url}")
                return url
        
        logger.debug(f"No Toast URL found for {competitor.name}")
        return None
    
    def _is_toast_menu_data(self, json_obj: Dict) -> bool:
        """Check if JSON-LD object contains Toast menu data"""
        if not isinstance(json_obj, dict):
            return False
        
        # Look for Toast-specific indicators
        obj_type = json_obj.get('@type', '')
        if obj_type in ['Menu', 'Restaurant', 'FoodEstablishment']:
            return True
        
        # Check for Toast-specific fields
        if 'hasMenu' in json_obj or 'menu' in json_obj:
            return True
        
        return False
    
    def _parse_toast_json_ld(self, json_obj: Dict) -> Optional[Dict]:
        """Parse Toast JSON-LD menu data"""
        try:
            menu_data = {'categories': []}
            
            # Extract menu from different JSON-LD structures
            menu_sections = []
            
            if 'hasMenu' in json_obj:
                menu_sections = json_obj['hasMenu']
            elif 'menu' in json_obj:
                menu_sections = json_obj['menu']
            elif '@type' == 'Menu':
                menu_sections = [json_obj]
            
            if not isinstance(menu_sections, list):
                menu_sections = [menu_sections]
            
            for section in menu_sections:
                if not isinstance(section, dict):
                    continue
                
                category = self._parse_toast_menu_section(section)
                if category and category.get('items'):
                    menu_data['categories'].append(category)
            
            return menu_data if menu_data['categories'] else None
            
        except Exception as e:
            logger.error(f"Error parsing Toast JSON-LD: {e}")
            return None
    
    def _parse_toast_menu_section(self, section: Dict) -> Optional[Dict]:
        """Parse a single Toast menu section"""
        try:
            category = {
                'name': section.get('name', 'Menu Items'),
                'items': []
            }
            
            # Extract items from section
            items = section.get('hasMenuSection', []) or section.get('menuItem', []) or section.get('items', [])
            
            if not isinstance(items, list):
                items = [items]
            
            for item in items:
                if not isinstance(item, dict):
                    continue
                
                menu_item = self._parse_toast_menu_item(item)
                if menu_item:
                    category['items'].append(menu_item)
            
            return category
            
        except Exception as e:
            logger.error(f"Error parsing Toast menu section: {e}")
            return None
    
    def _parse_toast_menu_item(self, item: Dict) -> Optional[Dict]:
        """Parse a single Toast menu item"""
        try:
            name = item.get('name')
            if not name:
                return None
            
            # Extract price from various possible structures
            price = None
            
            # Try different price field names
            price_fields = ['price', 'offers', 'priceSpecification']
            for field in price_fields:
                if field in item:
                    price_data = item[field]
                    if isinstance(price_data, dict):
                        price = price_data.get('price') or price_data.get('amount')
                    elif isinstance(price_data, (int, float, str)):
                        price = price_data
                    
                    if price:
                        break
            
            # Clean and validate price
            if price:
                price = menu_utils.clean_price_string(str(price))
            
            menu_item = {
                'name': str(name).strip(),
                'price': price
            }
            
            # Add description if available
            description = item.get('description')
            if description:
                menu_item['description'] = str(description).strip()
            
            return menu_item
            
        except Exception as e:
            logger.error(f"Error parsing Toast menu item: {e}")
            return None
    
    def _parse_toast_html(self, soup: BeautifulSoup) -> Optional[Dict]:
        """Parse Toast menu from HTML structure"""
        try:
            menu_data = {'categories': []}
            
            # Toast often uses specific CSS classes for menu items
            toast_selectors = [
                '.menu-section',
                '.menu-category', 
                '.toast-menu-section',
                '[data-testid*="menu"]',
                '.menu-item-container'
            ]
            
            for selector in toast_selectors:
                sections = soup.select(selector)
                if sections:
                    logger.debug(f"Found Toast menu sections with selector: {selector}")
                    
                    for section in sections:
                        category = self._parse_toast_html_section(section)
                        if category and category.get('items'):
                            menu_data['categories'].append(category)
                    
                    if menu_data['categories']:
                        return menu_data
            
            return None
            
        except Exception as e:
            logger.error(f"Error parsing Toast HTML: {e}")
            return None
    
    def _parse_toast_html_section(self, section) -> Optional[Dict]:
        """Parse a Toast HTML menu section"""
        try:
            # Extract category name
            category_name = "Menu Items"
            
            name_selectors = ['h2', 'h3', '.section-title', '.category-name', '.menu-section-title']
            for selector in name_selectors:
                name_elem = section.select_one(selector)
                if name_elem and name_elem.get_text(strip=True):
                    category_name = name_elem.get_text(strip=True)
                    break
            
            category = {
                'name': category_name,
                'items': []
            }
            
            # Extract menu items
            item_selectors = ['.menu-item', '.toast-menu-item', '[data-testid*="item"]']
            
            for selector in item_selectors:
                items = section.select(selector)
                if items:
                    for item_elem in items:
                        item = self._parse_toast_html_item(item_elem)
                        if item:
                            category['items'].append(item)
                    break
            
            return category
            
        except Exception as e:
            logger.error(f"Error parsing Toast HTML section: {e}")
            return None
    
    def _parse_toast_html_item(self, item_elem) -> Optional[Dict]:
        """Parse a single Toast HTML menu item"""
        try:
            # Extract item name
            name = None
            name_selectors = ['.item-name', '.menu-item-name', 'h4', 'h5', '.title']
            
            for selector in name_selectors:
                name_elem = item_elem.select_one(selector)
                if name_elem and name_elem.get_text(strip=True):
                    name = name_elem.get_text(strip=True)
                    break
            
            if not name:
                return None
            
            # Extract price
            price = None
            price_selectors = ['.price', '.item-price', '.menu-item-price', '[data-testid*="price"]']
            
            for selector in price_selectors:
                price_elem = item_elem.select_one(selector)
                if price_elem and price_elem.get_text(strip=True):
                    price_text = price_elem.get_text(strip=True)
                    price = menu_utils.clean_price_string(price_text)
                    if price:
                        break
            
            menu_item = {
                'name': name,
                'price': price
            }
            
            # Extract description
            desc_selectors = ['.description', '.item-description', '.menu-item-description']
            for selector in desc_selectors:
                desc_elem = item_elem.select_one(selector)
                if desc_elem and desc_elem.get_text(strip=True):
                    menu_item['description'] = desc_elem.get_text(strip=True)
                    break
            
            return menu_item
            
        except Exception as e:
            logger.error(f"Error parsing Toast HTML item: {e}")
            return None
    
    def _extract_toast_api_data(self, soup: BeautifulSoup) -> Optional[Dict]:
        """Extract menu data from Toast API calls in script tags"""
        try:
            # Look for script tags that might contain API data
            scripts = soup.find_all('script')
            
            for script in scripts:
                if not script.string:
                    continue
                
                script_content = script.string
                
                # Look for menu-related API data
                if 'menu' in script_content.lower() and '{' in script_content:
                    # Try to extract JSON objects
                    json_matches = re.findall(r'\{[^{}]*"menu"[^{}]*\}', script_content)
                    
                    for match in json_matches:
                        try:
                            data = json.loads(match)
                            if 'menu' in data:
                                return self._parse_toast_api_menu(data['menu'])
                        except json.JSONDecodeError:
                            continue
            
            return None
            
        except Exception as e:
            logger.error(f"Error extracting Toast API data: {e}")
            return None
    
    def _parse_toast_api_menu(self, menu_data) -> Optional[Dict]:
        """Parse menu data from Toast API response"""
        try:
            if not isinstance(menu_data, (dict, list)):
                return None
            
            menu = {'categories': []}
            
            if isinstance(menu_data, list):
                sections = menu_data
            else:
                sections = menu_data.get('sections', [menu_data])
            
            for section in sections:
                if isinstance(section, dict):
                    category = self._parse_toast_menu_section(section)
                    if category and category.get('items'):
                        menu['categories'].append(category)
            
            return menu if menu['categories'] else None
            
        except Exception as e:
            logger.error(f"Error parsing Toast API menu: {e}")
            return None
    
    # ============================================================================
    # SQUARE EXTRACTION (SECOND PRIORITY - 70% SUCCESS RATE)
    # ============================================================================
    
    async def _extract_via_square(self, competitor) -> Optional[Dict]:
        """
        Extract menu from Square website
        
        Square websites have various templates but often use structured data
        Success rate: 70% when Square URL is found
        Cost: $0.02 (HTTP requests only)
        """
        
        square_url = self._find_square_url(competitor)
        if not square_url:
            logger.debug(f"No Square URL found for {competitor.name}")
            return None
        
        logger.info(f"ðŸŸ¦ Found Square URL: {square_url}")
        
        try:
            # Fetch HTML content
            html = await menu_utils.fetch_html(square_url)
            if not html:
                logger.warning(f"Failed to fetch Square HTML for {competitor.name}")
                return None
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # Method 1: Try JSON-LD structured data
            json_ld_objects = menu_utils.extract_json_ld(html)
            for json_obj in json_ld_objects:
                if self._is_square_menu_data(json_obj):
                    menu = self._parse_square_json_ld(json_obj)
                    if menu:
                        logger.info(f"âœ… Square JSON-LD extraction successful")
                        return menu
            
            # Method 2: Parse Square HTML structure
            menu = self._parse_square_html(soup)
            if menu:
                logger.info(f"âœ… Square HTML extraction successful")
                return menu
            
            # Method 3: Look for Square-specific data attributes
            menu = self._extract_square_data_attributes(soup)
            if menu:
                logger.info(f"âœ… Square data attributes extraction successful")
                return menu
            
            logger.warning(f"Square extraction found URL but no menu data for {competitor.name}")
            return None
            
        except Exception as e:
            logger.error(f"Square extraction error for {competitor.name}: {e}")
            return None
    
    def _find_square_url(self, competitor) -> Optional[str]:
        """
        Find Square website URL for competitor
        
        Square URL patterns:
        - {restaurant}.square.site
        - {restaurant}restaurant.square.site
        - Custom domains using Square
        """
        
        # Method 1: Check if website is already a Square URL
        if hasattr(competitor, 'website') and competitor.website:
            if 'square.site' in competitor.website or 'squareup.com' in competitor.website:
                logger.debug(f"Found direct Square URL in website: {competitor.website}")
                return competitor.website
        
        # Method 2: Generate likely Square URL from restaurant name
        restaurant_slug = menu_utils.generate_restaurant_slug(competitor.name)
        if not restaurant_slug:
            return None
        
        # Common Square URL patterns
        square_patterns = [
            f"https://{restaurant_slug}.square.site",
            f"https://{restaurant_slug}restaurant.square.site",
            f"https://{restaurant_slug}pizza.square.site",  # For pizza places
            f"https://{restaurant_slug}-restaurant.square.site",
            f"https://{restaurant_slug}cafe.square.site",   # For cafes
        ]
        
        # Test each pattern
        for url in square_patterns:
            logger.debug(f"Testing Square URL: {url}")
            if menu_utils.url_exists(url):
                logger.info(f"âœ… Found working Square URL: {url}")
                return url
        
        logger.debug(f"No Square URL found for {competitor.name}")
        return None
    
    def _is_square_menu_data(self, json_obj: Dict) -> bool:
        """Check if JSON-LD object contains Square menu data"""
        if not isinstance(json_obj, dict):
            return False
        
        # Look for Square-specific indicators
        obj_type = json_obj.get('@type', '')
        if obj_type in ['Menu', 'Restaurant', 'FoodEstablishment']:
            return True
        
        # Check for Square-specific fields
        if 'hasMenu' in json_obj or 'menu' in json_obj or 'menuSection' in json_obj:
            return True
        
        return False
    
    def _parse_square_json_ld(self, json_obj: Dict) -> Optional[Dict]:
        """Parse Square JSON-LD menu data"""
        try:
            menu_data = {'categories': []}
            
            # Extract menu sections from different JSON-LD structures
            menu_sections = []
            
            if 'hasMenu' in json_obj:
                menu_sections = json_obj['hasMenu']
            elif 'menu' in json_obj:
                menu_sections = json_obj['menu']
            elif 'menuSection' in json_obj:
                menu_sections = json_obj['menuSection']
            elif json_obj.get('@type') == 'Menu':
                menu_sections = [json_obj]
            
            if not isinstance(menu_sections, list):
                menu_sections = [menu_sections]
            
            for section in menu_sections:
                if not isinstance(section, dict):
                    continue
                
                category = self._parse_square_menu_section(section)
                if category and category.get('items'):
                    menu_data['categories'].append(category)
            
            return menu_data if menu_data['categories'] else None
            
        except Exception as e:
            logger.error(f"Error parsing Square JSON-LD: {e}")
            return None
    
    def _parse_square_menu_section(self, section: Dict) -> Optional[Dict]:
        """Parse a single Square menu section"""
        try:
            category = {
                'name': section.get('name', 'Menu Items'),
                'items': []
            }
            
            # Extract items from section
            items = (section.get('hasMenuSection', []) or 
                    section.get('menuItem', []) or 
                    section.get('items', []) or
                    section.get('offers', []))
            
            if not isinstance(items, list):
                items = [items]
            
            for item in items:
                if not isinstance(item, dict):
                    continue
                
                menu_item = self._parse_square_menu_item(item)
                if menu_item:
                    category['items'].append(menu_item)
            
            return category
            
        except Exception as e:
            logger.error(f"Error parsing Square menu section: {e}")
            return None
    
    def _parse_square_menu_item(self, item: Dict) -> Optional[Dict]:
        """Parse a single Square menu item"""
        try:
            name = item.get('name')
            if not name:
                return None
            
            # Extract price from various Square structures
            price = None
            
            # Try different price field names used by Square
            price_fields = ['price', 'offers', 'priceSpecification', 'cost', 'amount']
            for field in price_fields:
                if field in item:
                    price_data = item[field]
                    if isinstance(price_data, dict):
                        price = (price_data.get('price') or 
                                price_data.get('amount') or 
                                price_data.get('value'))
                    elif isinstance(price_data, (int, float, str)):
                        price = price_data
                    
                    if price:
                        break
            
            # Clean and validate price
            if price:
                price = menu_utils.clean_price_string(str(price))
            
            menu_item = {
                'name': str(name).strip(),
                'price': price
            }
            
            # Add description if available
            description = item.get('description')
            if description:
                menu_item['description'] = str(description).strip()
            
            return menu_item
            
        except Exception as e:
            logger.error(f"Error parsing Square menu item: {e}")
            return None
    
    def _parse_square_html(self, soup: BeautifulSoup) -> Optional[Dict]:
        """Parse Square menu from HTML structure"""
        try:
            menu_data = {'categories': []}
            
            # Square uses various CSS classes depending on template
            square_selectors = [
                '.menu-section',
                '.square-menu-section',
                '.menu-category',
                '.product-category',
                '.item-category',
                '[data-automation-id*="menu"]',
                '.sqs-block-menu'
            ]
            
            for selector in square_selectors:
                sections = soup.select(selector)
                if sections:
                    logger.debug(f"Found Square menu sections with selector: {selector}")
                    
                    for section in sections:
                        category = self._parse_square_html_section(section)
                        if category and category.get('items'):
                            menu_data['categories'].append(category)
                    
                    if menu_data['categories']:
                        return menu_data
            
            # Fallback: Look for any structured menu-like content
            menu = self._parse_generic_square_structure(soup)
            if menu:
                return menu
            
            return None
            
        except Exception as e:
            logger.error(f"Error parsing Square HTML: {e}")
            return None
    
    def _parse_square_html_section(self, section) -> Optional[Dict]:
        """Parse a Square HTML menu section"""
        try:
            # Extract category name
            category_name = "Menu Items"
            
            name_selectors = [
                'h1', 'h2', 'h3', 'h4',
                '.section-title', 
                '.category-title',
                '.menu-section-title',
                '.product-category-title',
                '[data-automation-id*="title"]'
            ]
            
            for selector in name_selectors:
                name_elem = section.select_one(selector)
                if name_elem and name_elem.get_text(strip=True):
                    category_name = name_elem.get_text(strip=True)
                    break
            
            category = {
                'name': category_name,
                'items': []
            }
            
            # Extract menu items
            item_selectors = [
                '.menu-item',
                '.product-item', 
                '.square-menu-item',
                '.item',
                '[data-automation-id*="item"]',
                '.sqs-block-product'
            ]
            
            for selector in item_selectors:
                items = section.select(selector)
                if items:
                    for item_elem in items:
                        item = self._parse_square_html_item(item_elem)
                        if item:
                            category['items'].append(item)
                    break
            
            return category
            
        except Exception as e:
            logger.error(f"Error parsing Square HTML section: {e}")
            return None
    
    def _parse_square_html_item(self, item_elem) -> Optional[Dict]:
        """Parse a single Square HTML menu item"""
        try:
            # Extract item name
            name = None
            name_selectors = [
                '.item-name', 
                '.product-name',
                '.menu-item-name',
                '.title',
                'h3', 'h4', 'h5',
                '[data-automation-id*="name"]'
            ]
            
            for selector in name_selectors:
                name_elem = item_elem.select_one(selector)
                if name_elem and name_elem.get_text(strip=True):
                    name = name_elem.get_text(strip=True)
                    break
            
            if not name:
                return None
            
            # Extract price
            price = None
            price_selectors = [
                '.price', 
                '.item-price',
                '.product-price',
                '.menu-item-price',
                '.cost',
                '[data-automation-id*="price"]',
                '.sqs-money-native'
            ]
            
            for selector in price_selectors:
                price_elem = item_elem.select_one(selector)
                if price_elem and price_elem.get_text(strip=True):
                    price_text = price_elem.get_text(strip=True)
                    price = menu_utils.clean_price_string(price_text)
                    if price:
                        break
            
            menu_item = {
                'name': name,
                'price': price
            }
            
            # Extract description
            desc_selectors = [
                '.description', 
                '.item-description',
                '.product-description',
                '.menu-item-description',
                '[data-automation-id*="description"]'
            ]
            
            for selector in desc_selectors:
                desc_elem = item_elem.select_one(selector)
                if desc_elem and desc_elem.get_text(strip=True):
                    menu_item['description'] = desc_elem.get_text(strip=True)
                    break
            
            return menu_item
            
        except Exception as e:
            logger.error(f"Error parsing Square HTML item: {e}")
            return None
    
    def _extract_square_data_attributes(self, soup: BeautifulSoup) -> Optional[Dict]:
        """Extract menu data from Square data attributes"""
        try:
            # Look for Square-specific data attributes
            elements_with_data = soup.find_all(attrs={'data-automation-id': True})
            
            menu_data = {'categories': []}
            current_category = None
            
            for elem in elements_with_data:
                automation_id = elem.get('data-automation-id', '')
                
                if 'menu' in automation_id.lower() or 'category' in automation_id.lower():
                    # This might be a category
                    category_name = elem.get_text(strip=True)
                    if category_name:
                        current_category = {
                            'name': category_name,
                            'items': []
                        }
                        menu_data['categories'].append(current_category)
                
                elif 'item' in automation_id.lower() or 'product' in automation_id.lower():
                    # This might be a menu item
                    if current_category is not None:
                        item = self._parse_square_html_item(elem)
                        if item:
                            current_category['items'].append(item)
            
            return menu_data if menu_data['categories'] else None
            
        except Exception as e:
            logger.error(f"Error extracting Square data attributes: {e}")
            return None
    
    def _parse_generic_square_structure(self, soup: BeautifulSoup) -> Optional[Dict]:
        """Fallback parser for generic Square website structure"""
        try:
            menu_data = {'categories': []}
            
            # Look for any elements that might contain menu items
            # Square sites often have prices in specific formats
            price_pattern = re.compile(r'\$\d+\.?\d*')
            
            # Find all elements with prices
            elements_with_prices = []
            for elem in soup.find_all(text=price_pattern):
                parent = elem.parent
                if parent:
                    elements_with_prices.append(parent)
            
            if len(elements_with_prices) >= 3:  # Need at least 3 items for a valid menu
                category = {
                    'name': 'Menu Items',
                    'items': []
                }
                
                for elem in elements_with_prices:
                    # Try to extract item name and price from this element
                    text = elem.get_text(strip=True)
                    
                    # Look for price in text
                    price_match = price_pattern.search(text)
                    if price_match:
                        price_str = price_match.group()
                        price = menu_utils.clean_price_string(price_str)
                        
                        # Extract name (text before price)
                        name = text[:price_match.start()].strip()
                        if name and len(name) > 2:  # Valid name
                            category['items'].append({
                                'name': name,
                                'price': price
                            })
                
                if category['items']:
                    menu_data['categories'].append(category)
            
            return menu_data if menu_data['categories'] else None
            
        except Exception as e:
            logger.error(f"Error parsing generic Square structure: {e}")
            return None
    
    # ============================================================================
    # SLICE EXTRACTION (THIRD PRIORITY - 80% SUCCESS RATE)
    # ============================================================================
    
    async def _extract_via_slice(self, competitor) -> Optional[Dict]:
        """
        Extract menu from Slice platform
        
        Slice has well-structured menu data and consistent HTML
        Success rate: 80% when Slice URL is found
        Cost: $0.03 (slightly more complex parsing)
        """
        
        slice_url = self._find_slice_url(competitor)
        if not slice_url:
            logger.debug(f"No Slice URL found for {competitor.name}")
            return None
        
        logger.info(f"ðŸ• Found Slice URL: {slice_url}")
        
        try:
            # Fetch HTML content
            html = await menu_utils.fetch_html(slice_url)
            if not html:
                logger.warning(f"Failed to fetch Slice HTML for {competitor.name}")
                return None
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # Method 1: Parse Slice HTML structure (most reliable)
            menu = self._parse_slice_html(soup)
            if menu:
                logger.info(f"âœ… Slice HTML extraction successful")
                return menu
            
            # Method 2: Look for Slice API data in script tags
            menu = self._extract_slice_api_data(soup)
            if menu:
                logger.info(f"âœ… Slice API data extraction successful")
                return menu
            
            logger.warning(f"Slice extraction found URL but no menu data for {competitor.name}")
            return None
            
        except Exception as e:
            logger.error(f"Slice extraction error for {competitor.name}: {e}")
            return None
    
    def _find_slice_url(self, competitor) -> Optional[str]:
        """
        Find Slice platform URL for competitor
        
        Slice URL patterns:
        - slicelife.com/restaurants/{state}/{city}/{zip}/{restaurant-slug}/menu
        - slice.com/r/{restaurant-id}
        """
        
        # Method 1: Check if website is already a Slice URL
        if hasattr(competitor, 'website') and competitor.website:
            if 'slice' in competitor.website.lower():
                logger.debug(f"Found direct Slice URL in website: {competitor.website}")
                return competitor.website
        
        # Method 2: Generate Slice URL from location and name
        if not hasattr(competitor, 'address') or not competitor.address:
            return None
        
        try:
            # Parse address to get city and state
            address_parts = competitor.address.split(',')
            if len(address_parts) < 2:
                return None
            
            city = address_parts[0].strip().lower().replace(' ', '-')
            state_part = address_parts[1].strip().lower()
            
            # Extract state abbreviation
            state = state_part.split()[0] if state_part else ''
            
            # Generate restaurant slug
            restaurant_slug = menu_utils.generate_restaurant_slug(competitor.name)
            if not restaurant_slug:
                return None
            
            # Try different Slice URL patterns
            slice_patterns = [
                f"https://slicelife.com/restaurants/{state}/{city}/02895/{restaurant_slug}/menu",
                f"https://slicelife.com/restaurants/{state}/{city}/{restaurant_slug}/menu",
                f"https://slice.com/restaurants/{state}/{city}/{restaurant_slug}",
            ]
            
            # Test each pattern
            for url in slice_patterns:
                logger.debug(f"Testing Slice URL: {url}")
                if menu_utils.url_exists(url):
                    logger.info(f"âœ… Found working Slice URL: {url}")
                    return url
            
        except Exception as e:
            logger.debug(f"Error generating Slice URL for {competitor.name}: {e}")
        
        logger.debug(f"No Slice URL found for {competitor.name}")
        return None
    
    def _parse_slice_html(self, soup: BeautifulSoup) -> Optional[Dict]:
        """Parse Slice menu from HTML structure"""
        try:
            menu_data = {'categories': []}
            
            # Slice uses specific CSS classes for menu structure
            slice_selectors = [
                '.menu-section',
                '.slice-menu-section',
                '.menu-category',
                '[data-testid*="menu-section"]',
                '.category-section'
            ]
            
            for selector in slice_selectors:
                sections = soup.select(selector)
                if sections:
                    logger.debug(f"Found Slice menu sections with selector: {selector}")
                    
                    for section in sections:
                        category = self._parse_slice_html_section(section)
                        if category and category.get('items'):
                            menu_data['categories'].append(category)
                    
                    if menu_data['categories']:
                        return menu_data
            
            # Fallback: Look for menu items directly
            menu = self._parse_slice_items_directly(soup)
            if menu:
                return menu
            
            return None
            
        except Exception as e:
            logger.error(f"Error parsing Slice HTML: {e}")
            return None
    
    def _parse_slice_html_section(self, section) -> Optional[Dict]:
        """Parse a Slice HTML menu section"""
        try:
            # Extract category name
            category_name = "Menu Items"
            
            name_selectors = [
                'h2', 'h3', 'h4',
                '.section-title',
                '.category-title', 
                '.menu-section-title',
                '[data-testid*="section-title"]'
            ]
            
            for selector in name_selectors:
                name_elem = section.select_one(selector)
                if name_elem and name_elem.get_text(strip=True):
                    category_name = name_elem.get_text(strip=True)
                    break
            
            category = {
                'name': category_name,
                'items': []
            }
            
            # Extract menu items
            item_selectors = [
                '.menu-item',
                '.slice-menu-item',
                '.product-item',
                '[data-testid*="menu-item"]',
                '.item-card'
            ]
            
            for selector in item_selectors:
                items = section.select(selector)
                if items:
                    for item_elem in items:
                        item = self._parse_slice_html_item(item_elem)
                        if item:
                            category['items'].append(item)
                    break
            
            return category
            
        except Exception as e:
            logger.error(f"Error parsing Slice HTML section: {e}")
            return None
    
    def _parse_slice_html_item(self, item_elem) -> Optional[Dict]:
        """Parse a single Slice HTML menu item"""
        try:
            # Extract item name
            name = None
            name_selectors = [
                '.item-name',
                '.menu-item-name',
                '.product-name',
                '.title',
                'h4', 'h5',
                '[data-testid*="item-name"]'
            ]
            
            for selector in name_selectors:
                name_elem = item_elem.select_one(selector)
                if name_elem and name_elem.get_text(strip=True):
                    name = name_elem.get_text(strip=True)
                    break
            
            if not name:
                return None
            
            # Extract price
            price = None
            price_selectors = [
                '.price',
                '.item-price',
                '.menu-item-price',
                '.product-price',
                '[data-testid*="price"]',
                '.cost'
            ]
            
            for selector in price_selectors:
                price_elem = item_elem.select_one(selector)
                if price_elem and price_elem.get_text(strip=True):
                    price_text = price_elem.get_text(strip=True)
                    price = menu_utils.clean_price_string(price_text)
                    if price:
                        break
            
            menu_item = {
                'name': name,
                'price': price
            }
            
            # Extract description
            desc_selectors = [
                '.description',
                '.item-description',
                '.menu-item-description',
                '.product-description',
                '[data-testid*="description"]'
            ]
            
            for selector in desc_selectors:
                desc_elem = item_elem.select_one(selector)
                if desc_elem and desc_elem.get_text(strip=True):
                    menu_item['description'] = desc_elem.get_text(strip=True)
                    break
            
            return menu_item
            
        except Exception as e:
            logger.error(f"Error parsing Slice HTML item: {e}")
            return None
    
    def _parse_slice_items_directly(self, soup: BeautifulSoup) -> Optional[Dict]:
        """Fallback: Parse Slice menu items directly without sections"""
        try:
            menu_data = {'categories': []}
            
            # Look for all menu items on the page
            item_selectors = [
                '.menu-item',
                '.slice-menu-item', 
                '.product-item',
                '[data-testid*="menu-item"]'
            ]
            
            all_items = []
            for selector in item_selectors:
                items = soup.select(selector)
                if items:
                    for item_elem in items:
                        item = self._parse_slice_html_item(item_elem)
                        if item:
                            all_items.append(item)
                    break
            
            if all_items:
                category = {
                    'name': 'Menu Items',
                    'items': all_items
                }
                menu_data['categories'].append(category)
            
            return menu_data if menu_data['categories'] else None
            
        except Exception as e:
            logger.error(f"Error parsing Slice items directly: {e}")
            return None
    
    def _extract_slice_api_data(self, soup: BeautifulSoup) -> Optional[Dict]:
        """Extract menu data from Slice API calls in script tags"""
        try:
            # Look for script tags that might contain Slice API data
            scripts = soup.find_all('script')
            
            for script in scripts:
                if not script.string:
                    continue
                
                script_content = script.string
                
                # Look for menu-related data in various formats
                if 'menu' in script_content.lower() and ('{' in script_content or '[' in script_content):
                    
                    # Try to find JSON objects with menu data
                    json_patterns = [
                        r'"menu":\s*(\{[^}]+\})',
                        r'"items":\s*(\[[^\]]+\])',
                        r'"categories":\s*(\[[^\]]+\])'
                    ]
                    
                    for pattern in json_patterns:
                        matches = re.findall(pattern, script_content)
                        for match in matches:
                            try:
                                data = json.loads(match)
                                menu = self._parse_slice_api_menu(data)
                                if menu:
                                    return menu
                            except json.JSONDecodeError:
                                continue
            
            return None
            
        except Exception as e:
            logger.error(f"Error extracting Slice API data: {e}")
            return None
    
    def _parse_slice_api_menu(self, menu_data) -> Optional[Dict]:
        """Parse menu data from Slice API response"""
        try:
            if not isinstance(menu_data, (dict, list)):
                return None
            
            menu = {'categories': []}
            
            if isinstance(menu_data, list):
                # List of items
                if menu_data:
                    category = {
                        'name': 'Menu Items',
                        'items': []
                    }
                    
                    for item_data in menu_data:
                        if isinstance(item_data, dict):
                            item = self._parse_slice_api_item(item_data)
                            if item:
                                category['items'].append(item)
                    
                    if category['items']:
                        menu['categories'].append(category)
            
            elif isinstance(menu_data, dict):
                # Dictionary with categories or items
                if 'categories' in menu_data:
                    categories = menu_data['categories']
                elif 'items' in menu_data:
                    categories = [{'name': 'Menu Items', 'items': menu_data['items']}]
                else:
                    categories = [menu_data]
                
                for cat_data in categories:
                    if isinstance(cat_data, dict):
                        category = {
                            'name': cat_data.get('name', 'Menu Items'),
                            'items': []
                        }
                        
                        items = cat_data.get('items', [])
                        for item_data in items:
                            if isinstance(item_data, dict):
                                item = self._parse_slice_api_item(item_data)
                                if item:
                                    category['items'].append(item)
                        
                        if category['items']:
                            menu['categories'].append(category)
            
            return menu if menu['categories'] else None
            
        except Exception as e:
            logger.error(f"Error parsing Slice API menu: {e}")
            return None
    
    def _parse_slice_api_item(self, item_data: Dict) -> Optional[Dict]:
        """Parse a single item from Slice API data"""
        try:
            name = item_data.get('name')
            if not name:
                return None
            
            # Extract price
            price = None
            price_fields = ['price', 'cost', 'amount', 'value']
            for field in price_fields:
                if field in item_data:
                    price = menu_utils.clean_price_string(str(item_data[field]))
                    if price:
                        break
            
            menu_item = {
                'name': str(name).strip(),
                'price': price
            }
            
            # Add description if available
            description = item_data.get('description')
            if description:
                menu_item['description'] = str(description).strip()
            
            return menu_item
            
        except Exception as e:
            logger.error(f"Error parsing Slice API item: {e}")
            return None
    
    # ============================================================================
    # GOOGLE VISION EXTRACTION (LAST RESORT - 95% SUCCESS RATE)
    # ============================================================================
    
    async def _extract_via_vision(self, competitor) -> Optional[Dict]:
        """
        Extract menu from photos using Google Vision API + Gemini LLM
        
        This is the most expensive but most reliable fallback method
        Success rate: 95% when menu photos are available
        Cost: $0.04 (Vision API + LLM structuring)
        """
        
        if not self.gemini_model:
            logger.warning(f"Gemini API not available for Vision extraction")
            return None
        
        logger.info(f"ðŸ“¸ Attempting Vision extraction for {competitor.name}")
        
        try:
            # Get menu photos from Google Places
            menu_photos = self._get_menu_photos(competitor)
            if not menu_photos:
                logger.debug(f"No menu photos found for {competitor.name}")
                return None
            
            logger.info(f"ðŸ“· Found {len(menu_photos)} potential menu photos")
            
            # Process the first (best) menu photo
            menu_text = await self._extract_text_from_photo(menu_photos[0])
            if not menu_text:
                logger.warning(f"Failed to extract text from menu photo")
                return None
            
            logger.info(f"ðŸ“ Extracted {len(menu_text)} characters of text from photo")
            
            # Use Gemini to structure the extracted text into menu format
            structured_menu = await self._structure_menu_text_with_llm(menu_text, competitor.name)
            if structured_menu:
                logger.info(f"âœ… Vision extraction successful")
                return structured_menu
            
            logger.warning(f"Failed to structure menu text with LLM")
            return None
            
        except Exception as e:
            logger.error(f"Vision extraction error for {competitor.name}: {e}")
            return None
    
    def _get_menu_photos(self, competitor) -> List[str]:
        """
        Get menu photo URLs from Google Places API
        
        Uses the same Google API key as Gemini
        """
        try:
            # Import Google Places client (reuse existing setup)
            from services.google_places_service import GooglePlacesService
            
            places_service = GooglePlacesService()
            
            # Get place details with photos
            place_details = places_service.get_place_details(competitor.place_id)
            if not place_details or 'photos' not in place_details:
                logger.debug(f"No photos found in place details for {competitor.name}")
                return []
            
            photos = place_details['photos']
            menu_photos = []
            
            # Filter for likely menu photos (first 5 photos are usually best)
            for i, photo in enumerate(photos[:5]):
                photo_reference = photo.get('photo_reference')
                if photo_reference:
                    # Generate photo URL
                    photo_url = f"https://maps.googleapis.com/maps/api/place/photo?maxwidth=800&photoreference={photo_reference}&key={self.gemini_api_key}"
                    menu_photos.append(photo_url)
            
            logger.debug(f"Found {len(menu_photos)} photos for {competitor.name}")
            return menu_photos
            
        except Exception as e:
            logger.error(f"Error getting menu photos for {competitor.name}: {e}")
            return []
    
    async def _extract_text_from_photo(self, photo_url: str) -> Optional[str]:
        """
        Extract text from menu photo using Google Vision API
        
        Note: Using Gemini's vision capabilities instead of separate Vision API
        """
        try:
            logger.info(f"ðŸ” Extracting text from photo: {photo_url}")
            
            # Use Gemini's vision capabilities to extract text
            # This is more cost-effective than separate Vision API
            
            prompt = """
            Extract all text from this menu image. 
            Return the raw text exactly as it appears, preserving:
            - Item names
            - Prices 
            - Categories/sections
            - Descriptions
            
            Do not format or structure the text, just extract it as-is.
            """
            
            # Create image part for Gemini
            import PIL.Image
            import requests
            from io import BytesIO
            
            # Download image
            response = requests.get(photo_url, timeout=10)
            response.raise_for_status()
            
            # Convert to PIL Image
            image = PIL.Image.open(BytesIO(response.content))
            
            # Use Gemini to extract text
            response = self.gemini_model.generate_content([prompt, image])
            
            if response and response.text:
                extracted_text = response.text.strip()
                logger.info(f"âœ… Successfully extracted {len(extracted_text)} characters")
                return extracted_text
            
            return None
            
        except Exception as e:
            logger.error(f"Error extracting text from photo: {e}")
            return None
    
    async def _structure_menu_text_with_llm(self, menu_text: str, competitor_name: str) -> Optional[Dict]:
        """
        Use Gemini LLM to structure extracted menu text into standardized format
        """
        try:
            logger.info(f"ðŸ§  Structuring menu text with LLM for {competitor_name}")
            
            prompt = f"""
            Convert this raw menu text into structured JSON format.
            
            Restaurant: {competitor_name}
            
            Raw menu text:
            {menu_text}
            
            Convert to this exact JSON structure:
            {{
                "categories": [
                    {{
                        "name": "Category Name",
                        "items": [
                            {{
                                "name": "Item Name",
                                "price": 12.99,
                                "description": "Optional description"
                            }}
                        ]
                    }}
                ]
            }}
            
            Rules:
            - Extract all menu items with names and prices
            - Group items into logical categories (Pizza, Salads, etc.)
            - Convert prices to numbers (remove $ symbols)
            - Include descriptions if clearly associated with items
            - If no clear categories, use "Menu Items" as category name
            - Only include items that have both name and price
            - Return valid JSON only, no other text
            """
            
            response = self.gemini_model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.1,  # Low temperature for consistent formatting
                    max_output_tokens=2000,
                    response_mime_type="application/json"
                )
            )
            
            if response and response.text:
                try:
                    # Parse the JSON response
                    structured_menu = json.loads(response.text)
                    
                    # Validate the structure
                    if menu_utils.validate_menu_quality(structured_menu):
                        logger.info(f"âœ… Successfully structured menu with LLM")
                        return structured_menu
                    else:
                        logger.warning(f"LLM structured menu failed quality validation")
                        return None
                        
                except json.JSONDecodeError as e:
                    logger.error(f"Failed to parse LLM JSON response: {e}")
                    return None
            
            return None
            
        except Exception as e:
            logger.error(f"Error structuring menu text with LLM: {e}")
            return None
    

    
    # ============================================================================
    # CACHING METHODS (IMPLEMENTED)
    # ============================================================================
    
    async def get_cached_menu(self, competitor_id: str) -> Optional[Dict]:
        """Get menu from cache if available and not expired"""
        try:
            from database.supabase_client import get_supabase_service_client
            
            supabase = get_supabase_service_client()
            
            # Query for non-expired cached menu
            result = supabase.table("competitor_menus").select("*").eq(
                "competitor_place_id", competitor_id
            ).gt("expires_at", datetime.now().isoformat()).execute()
            
            if result.data:
                cached_menu = result.data[0]
                logger.info(f"âœ… Found cached menu for {competitor_id}")
                
                # Update last accessed time
                supabase.table("competitor_menus").update({
                    "last_accessed_at": datetime.now().isoformat()
                }).eq("id", cached_menu["id"]).execute()
                
                return {
                    'competitor_name': cached_menu['competitor_name'],
                    'competitor_id': competitor_id,
                    'extraction_method': cached_menu['extraction_method'],
                    'success': True,
                    'menu_data': cached_menu['menu_data'],
                    'extracted_at': cached_menu['created_at'],
                    'item_count': cached_menu.get('item_count', 0),
                    'cached': True
                }
            
            return None
            
        except Exception as e:
            logger.error(f"Error getting cached menu: {e}")
            return None
    
    async def cache_menu(self, competitor_id: str, competitor_name: str, menu_data: Dict, extraction_method: str, extraction_url: str = None):
        """Cache extracted menu with 7-day TTL"""
        try:
            from database.supabase_client import get_supabase_service_client
            
            supabase = get_supabase_service_client()
            
            cache_data = {
                "competitor_place_id": competitor_id,
                "competitor_name": competitor_name,
                "menu_data": menu_data,
                "extraction_method": extraction_method,
                "extraction_url": extraction_url,
                "item_count": self._count_menu_items(menu_data),
                "success_rate": 1.0,
                "created_at": datetime.now().isoformat(),
                "expires_at": (datetime.now() + timedelta(days=7)).isoformat()
            }
            
            # Upsert (insert or update if exists)
            supabase.table("competitor_menus").upsert(cache_data).execute()
            
            logger.info(f"âœ… Cached menu for {competitor_name} ({extraction_method})")
            
        except Exception as e:
            logger.error(f"Error caching menu: {e}")
    
    # ============================================================================
    # QUALITY ASSESSMENT (IMPLEMENTED)
    # ============================================================================
    
    def assess_menu_quality(self, menu_data: Dict) -> float:
        """
        Assess quality of extracted menu (0.0-1.0)
        
        Factors:
        - Number of items with prices
        - Category organization
        - Description completeness
        - Price reasonableness
        """
        if not menu_data or not isinstance(menu_data, dict):
            return 0.0
        
        categories = menu_data.get('categories', [])
        if not categories:
            return 0.0
        
        total_items = 0
        items_with_prices = 0
        items_with_descriptions = 0
        reasonable_prices = 0
        
        for category in categories:
            if not isinstance(category, dict):
                continue
            
            for item in category.get('items', []):
                if not isinstance(item, dict):
                    continue
                
                total_items += 1
                
                # Check if has price
                if item.get('price') is not None:
                    items_with_prices += 1
                    
                    # Check if price is reasonable
                    price = item.get('price')
                    if isinstance(price, (int, float)) and 1 <= price <= 100:
                        reasonable_prices += 1
                
                # Check if has description
                if item.get('description'):
                    items_with_descriptions += 1
        
        if total_items == 0:
            return 0.0
        
        # Calculate quality score
        price_score = items_with_prices / total_items
        description_score = items_with_descriptions / total_items * 0.3  # Less important
        reasonable_price_score = reasonable_prices / max(items_with_prices, 1) * 0.2
        category_score = min(len(categories) / 3, 1.0) * 0.2  # Bonus for good organization
        
        quality_score = price_score + description_score + reasonable_price_score + category_score
        
        return min(quality_score, 1.0)